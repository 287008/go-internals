# 5.3 设计与演化
其实讲一个东西，讲它是什么样是不足够的。如果能讲清楚它为什么会是这样子，则会举一反三。所以这里将从最基本的线程池讲起，谈谈Go调度设计背后的故事，讲清楚它为什么是这样子。

## 线程池
先看一些简单点的吧。一个常规的 线程池+任务队列 的模型如图所示：

![](images/5.3.worker.png?raw=true)

把每个工作线程叫worker的话，每条线程运行一个worker，每个worker做的事情就是不停地从队列中取出任务并执行：

```c
	while(!empty(queue)) {
	    q = get(queue); //从任务队列中取一个(涉及加锁等)
	    q->callback(); //执行该任务
	}
```

当然，这是最简单的情形，但是一个很明显的问题就是一个进入callback之后，就失去了控制权。因为没有一个调度器层的东西，一个任务可以执行很长很长时间一直占用的worker线程，或者阻塞于io之类的。

也许用Go语言表述会更地道一些。好吧，那么让我们用Go语言来描述。假设我们有一些“任务”，任务是一个可运行的东西，也就是只要满足Run函数，它就是一个任务。所以我们就把这个任务叫作接口G吧。

```go
	type G interface {
		Run() 
	}
```

我们有一个全局的任务队列，里面包含很多可运行的任务。线程池的各个线程从全局的任务队列中取任务时，显然是需要并发保护的，所以有下面这个结构体：

```go
	type Sched struct {
		allg  []G
		lock	*sync.Mutex
	}
```

以及它的变量

```go
	var sched Sched
```

每条线程是一个worker，这里我们给worker换个名字，就把它叫M吧。前面已经说过了，worker做的事情就是不停的去任务队列中取一个任务出来执行。于是用Go语言大概可以写成这样子：

```go
	func M() {
		for {
			sched.lock.Lock()	//互斥地从就绪G队列中取一个g出来运行
			if sched.allg > 0 {
				g := sched.allg[0]
				sched.allg = sched.allg[1:]
				sched.lock.Unlock()
				g.Run()		//运行它
			} else {
				sched.lock.Unlock()
			}
		}
	}
```

接下来，将整个系统启动：

```go
	for i:=0; i<GOMAXPROCS; i++ {
		go M()
	}
```

假定我们有一个满足G接口的main，然后它在自己的Run中不断地将新的任务挂到sched.allg中，这个线程池+任务队列的系统模型就会一直运行下去。

可以看到，这里在代码取中故意地用Go语言中的G，M，甚至包括GOMAXPROCS等取名字。其实本质上，Go语言的调度层无非就是这样一个工作模式的：几条物理线程，不停地取goroutine运行。

## go1.0
前面小节的代码中有一些小问题。其中一个问题就是Run会一直执行，在它结束之前不会返回到调用器层面。而Go语言中的goroutine是一个类似协程的概念。

协程一类的东西一般会提供类似yield的函数。协程运行到一定时候就主动调用yield放弃自己的执行，把自己再次放回到任务队列中等待下一次调用时机等等。

将一个正在执行的任务yield出去，再在某个时刻再弄回来继续运行，这就涉及到一个问题，即执行的上下文环境。其实Go语言中的goroutine就是协程。每个结构体G中有一个sched域就是用于保存自己上下文的。这样，这种"任务"就可以被换出去，再换进来。

现在我们的任务变成了这个样子的：

```c
struct G {
    Gobuf sched;
    byte *stack;
}
```

一个线程是一个worker，假如运行到阻塞了呢？那干事的家伙岂不就少了，解耦还是不够。所以不是一个worker对应一条线程的，Go语言中又引入了struct M这层抽象。m就是这里的worker，但不是线程。处理系统调用中的m不会占用线程,只有干事的m才会对应线程.

于是就变成了这样子:
[[file:image/m_g.jpg]]
然后就变成了线程的入口是mstart,而goroutine的入口是在schedule中m和g都满足之后切换上下文进入的.
只是由于要优化,所以会搞的更复杂一些.比如要重用内存空间所以会有gfree和mhead之类的东西.

## 问题
当前go的调度器限制了Go程序的并发度。profile显示，有14%是浪费在了是浪费在了runtime.futex()中。

1. 单个全局锁(Sched.Lock)用来保护所有的goroutine相关的操作(创建，完成，调度等)。
2. Goroutine切换。工作线程在各自之前切换goroutine，这导致延迟和额外的负担。每个M都必须可以执行任何的G
3. 内存缓存MCache是每个M的。内存缓存和其它缓存是关联到所有的M的，而事实上它本只需要关联到运行Go代码的M(阻塞于系统调用的M是不需要mcache的)。运行着Go代码的M和所有M的比例可能高达1:100。这导致过度的资源消耗
4. 过多的线程阻塞、非阻塞。系统调用时的工作线程频繁地阻塞，恢复，造成过多的负担

## go1.1

解决方式是引入Processor的概念，并在Processors之上实现工作流窃取的调度器。

M代表OS线程。P代表Go代码执行时需要的资源。当M执行Go代码时，它需要关联一个P，当M为idle或者在系统调用中时，它也需要P。
有刚好GOMAXPROCS个P。所有的P被组织为一个数组，工作流窃取需要这个条件。GOMAXPROCS的改变涉及到停止/启动the world来resize数组P的大小


mcache从M中移到P中。gfree和grunnable从sched中移到P中。

这样就将sched.atomic也去掉了。

当一个新的G创建或者现有的G变成runnable，它将一个runnable的goroutine推到当前的P。当P完成执行G，它将G从自己的runnable goroutine中pop出去。如果链为空，P会随机从其它P中窃取一半的可运行goroutine。

当M创建一个新G的时候，必须保证有另一个M来执行这个G。类似的，当一个M进入到系统调用时，必须保证有另一个M来执行G的代码。

2层自旋：关联了P的idle的M自旋寻找新的G
没有关联P的M自旋等待可用的P

最多有GOMAXPROCS个自旋的M。只要有第二类M时第一类M就不会阻塞。

死锁检测 检测是否所有的P都idle了
Some variables from sched are de-centralized and moved to P. Some variables from M are moved to P (the ones that relate to active execution of Go code).

